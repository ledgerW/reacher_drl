{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control with Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview  \n",
    "\n",
    "The agent implementation follows very closely the approach, network architectures, and hyperparameters found in the original DDPG paper mentioned below.  With regard to the environment, we have 20 \"agents\" acting simultaneously and independantly in 20 distinct instances of the same environment.  As a practical matter, we really have only 1 instance of our DDPG agent, and therefore only 1 set of networks (local and target for both actor and critic) to be updated and one common replay buffer of experiences being \"shared\" among the agents.  So our agent takes a distinct action for each environment's state (20 at each timestep) and then records all 20 of those experiences/transitions into the replay buffer.  This lets us speed up the learning and exploration process.  In addition, we add a random noise element to each of the agent's actions, which helps to broaden the exploration space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm\n",
    "\n",
    "The Deep Deterministic Policy Gradient (DDPG) algorithm used in this repo is based on work first proposed in a paper titled, _Continuous Control With Deep Reinforcement Learning_. Nearly all of the architecture and hyperparameters are identical to those found in the paper, with the exception of update frequency and intervals.\n",
    "\n",
    "A few key aspects of DDPG outlined in the paper are listed below: \n",
    "\n",
    "- DDPG is an approach that combines the strengths and stability techniques of DQN, such as use of a replay buffer and target networks, with the actor-critic method that allows DPG to handle continuous action spaces.   \n",
    "  \n",
    "  \n",
    "- The actor network serves to map the state space to the specific (continuous) action to take, \"skiping\" the Q value approximation that happens with DQN.  The actor network is then updated using the policy gradient listed below.  \n",
    "  \n",
    "  \n",
    "- The critic network then serves to \"evaluate\" the actor network's action by estimating the Q value of the state and action given by the actor network.  With a Q value, we can now measure the Bellman Loss (also given below), which tells our agent to \"chase\" or \"avoid\" the Q value one timestep in the future if it's higher or lower than the current Q value, respectively.\n",
    "  \n",
    "  \n",
    "- Below is the loss function used to update the critic network.\n",
    "![loss-function](images/critic_loss_func.PNG)  \n",
    "  \n",
    "  \n",
    "- Below is the policy gradient used to update the actor network.\n",
    "![loss-function](images/actor_loss_func.PNG)\n",
    "  \n",
    "  \n",
    "- The DDPG algorithm pseudocode is given below.  \n",
    "![algorithm](images/ddpg_algo.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Network Achitecture\n",
    "\n",
    "The architectures in this implementation is as follows:\n",
    "\n",
    "**Actor**  \n",
    "Input:     Linear(num_units = state size) > batch norm  \n",
    "Hidden 1:  Linear(num_units = 400) > relu > batch norm   \n",
    "Hidden 2:  Linear(num_units = 300) > relu > batch norm  \n",
    "Output:    Linear(num_units = action size) > tanh  \n",
    "\n",
    "**Critic**  \n",
    "Input:     Linear(num_units = state size) > batch norm  \n",
    "Hidden 1:  Linear(num_units = 400) > relu > batch norm  \n",
    "Concat:    Concat(Critic-hidden1-output, Actor-output)  \n",
    "Hidden 2:  Linear(num_units = 300 + action_size) > relu > batch norm  \n",
    "Output:    Linear(num_units = 1)  \n",
    "\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "|Hyperparameter|Value|Description|\n",
    "|--------------:|----:|:-----------|\n",
    "|minibatch size| 64 | Number of training examples to sample from memory|\n",
    "|replay buffer|1000000|Number of experiences to store in memory|\n",
    "|gamma|0.99|Discount factor of gamma used in Q-learning update|\n",
    "|update frequency|20|how many timesteps between agent updates|\n",
    "|n updates|10|how many network updates to perform per agent update|\n",
    "|actor learning rate|1e-4|The learning rate used by Adam|\n",
    "|critic learning rate|3e-4|The learning rate used by Adam|\n",
    "|tau|1e-3|The parameters used by soft update of target network weights|\n",
    "|L2 weight decay|0|weight decay used by Adam|\n",
    "|max timesteps|1000|max number of timesteps for each episode|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Rewards  \n",
    "\n",
    "The plot below illustrates the agent's performance over time (or number of episodes). In the case of Reacher, the environment is considered solved when the agent acheives an average score of at least 30.0 over 100 consecutive episodes. We can see that our **DDPG agent was able to solve the environment after 134 episodes**\n",
    "\n",
    "![Performance](images/ddpg_results.PNG)\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Future Work\n",
    "\n",
    "Additional thoughts for future improvements that have been identified in literature and would be logical next steps to improve the agent, include:  \n",
    "\n",
    "1. **Priortized Experience Replay**: rather than a uniform random selection of experiences to learn from, we would assign weighted probabilities to the experiences in order to prioritize experences with the greater losses, for example...  \n",
    "  \n",
    "2. **Recurrent DDPG**: Very similar to the current DDPG implementation, but with a recurrent network architecture used for the actor.  \n",
    "  \n",
    "3. **Other Continuous Control Algorithms**: A few that have shown strong performance with continuous control tasks include, *TRPO*, *PPO*, and *D4PG*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
