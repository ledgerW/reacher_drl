{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control with Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm\n",
    "\n",
    "The Deep Deterministic Policy Gradient (DDPG) algorithm used in this repo is based on work first proposed in a paper titled, _Continuous Control With Deep Reinforcement Learning_. Nearly all of the architecture and hyperparameters are identical to those found in the paper, with the exception of update frequency and intervals.\n",
    "\n",
    "A few key aspects of DDPG outlined in the paper are listed below: \n",
    "\n",
    "- DDPG is an approach that combines the strengths and stability techniques of DQN, such as use of a replay buffer and target networks, with the actor-critic method that allows DPG to handle continuous action spaces. \n",
    "  \n",
    "  \n",
    "- Below is the loss function used to update the critic network.\n",
    "![loss-function](images/critic_loss_func.PNG)  \n",
    "  \n",
    "  \n",
    "- Below is the policy gradient used to update the actor network.\n",
    "![loss-function](images/actor_loss_func.PNG)\n",
    "  \n",
    "  \n",
    "- The DDPG algorithm pseudocode is given below.  \n",
    "![algorithm](images/ddpg_algo.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Network Achitecture\n",
    "\n",
    "The architectures in this implementation is as follows:\n",
    "\n",
    "**Actor**  \n",
    "Input:     Linear(num_units = state size) > batch norm  \n",
    "Hidden 1:  Linear(num_units = 400) > relu > batch norm   \n",
    "Hidden 2:  Linear(num_units = 300) > relu > batch norm  \n",
    "Output:    Linear(num_units = action size) > tanh  \n",
    "\n",
    "**Critic**  \n",
    "Input:     Linear(num_units = state size) > batch norm  \n",
    "Hidden 1:  Linear(num_units = 400) > relu > batch norm  \n",
    "Concat:    Concat(Critic-hidden1-output, Actor-output)  \n",
    "Hidden 2:  Linear(num_units = 300 + action_size) > relu > batch norm  \n",
    "Output:    Linear(num_units = 1)  \n",
    "\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "|Hyperparameter|Value|Description|\n",
    "|--------------:|----:|:-----------|\n",
    "|minibatch size| 512 | Number of training examples to sample from memory|\n",
    "|replay buffer|500000|Number of experiences to store in memory|\n",
    "|gamma|0.99|Discount factor of gamma used in Q-learning update|\n",
    "|update frequency|20|how many timesteps between agent updates|\n",
    "|n updates|10|how many network updates to perform per agent update|\n",
    "|actor learning rate|1e-4|The learning rate used by Adam|\n",
    "|critic learning rate|1e-3|The learning rate used by Adam|\n",
    "|tau|1e-3|The parameters used by soft update of target network weights|\n",
    "|L2 weight decay|0|weight decay used by Adam|\n",
    "|max training episodes|250|Number of episodes used for training|\n",
    "|max timesteps|1000|max number of timesteps for each episode|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Rewards  \n",
    "\n",
    "The plot below illustrates the agent's performance over time (or number of episodes). In the case of Reacher, the environment is considered solved when the agent acheives an average score of at least 30.0 over 100 consecutive episodes. We can see that our **DDPG agent was able to solve the environment after 133 episodes**\n",
    "\n",
    "![Performance](images/ddpg_results.PNG)\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Future Work\n",
    "\n",
    "Additional thoughts for future improvements that have been identified in literature and would be logical next steps to improve the agent, include:  \n",
    "\n",
    "1. **Priortized Experience Replay**: rather than a uniform random selection of experiences to learn from, we would assign weighted probabilities to the experiences in order to prioritize experences with the greater losses, for example...  \n",
    "  \n",
    "2. **Recurrent DDPG**: Very similar to the current DDPG implementation, cut with a recurrent network architecture used for the actor.  \n",
    "  \n",
    "3. **Other Continuous Control Algorithms**: A few that have shown strong performance with continuous control tasks include, *TRPO*, *PPO*, and *D4PG*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
